# Peer Review Management

**Related**: [MANUSCRIPT_REVISION_CHECKLIST.md](MANUSCRIPT_REVISION_CHECKLIST.md) | [reviews/README.md](reviews/README.md)
**Status**: Active
**Last Updated**: 2025-12-30

---

## Overview

This project supports both **synthetic** (AI-generated) and **actual** (journal) peer reviews with unified tracking and response management.

### Synthetic Reviews

Synthetic peer reviews generated by LLMs (Claude, GPT-4) stress-test methodology and identify weaknesses before journal submission:

1. Identify methodological gaps before reviewers do
2. Strengthen robustness checks and diagnostics
3. Improve manuscript clarity and framing
4. Anticipate likely reviewer concerns

### Actual Reviews

When you receive reviews from journals, track them using the same system with additional metadata:

- Journal and submission round tracking
- Reviewer identification (R1, R2, etc.)
- Decision tracking (major revision, minor revision, etc.)
- Git commit tracking for change auditing

---

## Workflow

### Step 1: Start Review Cycle

```bash
# SYNTHETIC REVIEWS (default)
# Using focus prompts (recommended)
python src/pipeline.py review_new --focus methods
python src/pipeline.py review_new --focus economics

# Multi-manuscript support
python src/pipeline.py review_new -m main -f policy

# ACTUAL REVIEWS (from journals)
python src/pipeline.py review_new --actual --journal "JEEM" --round "R&R1"
python src/pipeline.py review_new --actual -j "AER" -r "initial" --decision major_revision
python src/pipeline.py review_new --actual -j "QJE" -r "R&R1" --reviewers R1 R2 R3
```

**For synthetic reviews**, provide the LLM with:

- The manuscript abstract and key results
- Methodology description
- Main tables and figures
- Request a detailed methodological critique

**For actual reviews**, paste the reviewer comments into the tracker template.

### Step 2: Triage

Classify each review point using the status categories below.

```bash
# After manual triage, check status
python src/pipeline.py review_status

# For specific manuscript
python src/pipeline.py review_status -m main
```

### Step 3: Track

Document responses in:
- `manuscript_quarto/REVISION_TRACKER.md` - Detailed tracking
- `doc/MANUSCRIPT_REVISION_CHECKLIST.md` - High-level summary

### Step 4: Implement

For VALID - ACTION NEEDED items:
1. Create implementation plan
2. Modify code/run new analyses as needed
3. Update manuscript text
4. Verify changes address the concern

### Step 5: Verify

```bash
python src/pipeline.py review_verify
```

Complete verification checklist before closing review cycle.

---

## Status Classifications

| Status | Description | Action |
|--------|-------------|--------|
| **VALID - ACTION NEEDED** | Legitimate concern requiring changes | Implement fix, document response |
| **ALREADY ADDRESSED** | Concern already handled in manuscript | Point to existing section |
| **BEYOND SCOPE** | Valid but deferred (data unavailable, future work) | Document reason, note for future |
| **INVALID** | Reviewer misunderstanding or error | Clarify if manuscript is ambiguous |

### Detailed Classification Guidance

#### VALID - ACTION NEEDED
The concern has merit and should be addressed. Document:
- What specific changes will be made
- Which files will be modified
- How to verify the fix

#### ALREADY ADDRESSED
The concern is valid but already handled. Document:
- Specific section/line references where addressed
- Consider if current text is clear enough
- No code changes needed

#### BEYOND SCOPE
The concern is valid but cannot be addressed in current revision. Document:
- Why it's beyond scope (data unavailable, computational constraints, etc.)
- How it might be addressed in future work
- Any partial mitigation already in place

#### INVALID
The reviewer's concern is based on a misunderstanding. Document:
- What the reviewer misunderstood
- Correct interpretation
- Whether manuscript clarity should be improved

---

## Focus Prompt Templates

The review system supports both discipline-based and aspect-based focus prompts.

### Available Focus Areas

| Focus             | Type       | Description                                      |
| ----------------- | ---------- | ------------------------------------------------ |
| `economics`       | Discipline | Identification, causal inference, econometrics   |
| `engineering`     | Discipline | Reproducibility, benchmarks, validation          |
| `social_sciences` | Discipline | Theory, generalizability, ethics                 |
| `general`         | Discipline | Structure, clarity, contribution                 |
| `methods`         | Aspect     | Statistical rigor, methodology critique          |
| `policy`          | Aspect     | Practitioner perspective, actionability          |
| `clarity`         | Aspect     | Writing quality, accessibility                   |

### Economics/Finance

> "Act as a critical peer reviewer for a top economics journal (AER, QJE, JEEM, Econometrica).
>
> Review the following manuscript for:
> - **Identification strategy**: Is the causal claim credible? What are threats to identification?
> - **Econometric methods**: Are standard errors correctly specified? Is clustering appropriate?
> - **Pre-trend tests**: Are parallel trends assumptions tested and satisfied?
> - **Robustness checks**: Are alternative specifications explored?
> - **Data quality**: Are there measurement concerns? Selection issues?
> - **External validity**: How generalizable are findings?
>
> Be specific about threats to identification and suggest diagnostic tests.
> Format your response with numbered major and minor comments."

### Engineering/Technical

> "Act as a technical reviewer for a top engineering journal (IEEE, ASME, Nature Engineering).
>
> Review the following manuscript for:
> - **Methodology rigor**: Is the approach well-justified and correctly implemented?
> - **Reproducibility**: Can results be replicated from the description?
> - **Benchmark comparisons**: Are appropriate baselines used?
> - **Computational efficiency**: Is the approach scalable?
> - **Technical accuracy**: Are equations, algorithms, and implementations correct?
> - **Validation**: Is the experimental design sufficient to support claims?
>
> Identify gaps in experimental design and validation.
> Format your response with numbered major and minor comments."

### Social Sciences

> "Act as a reviewer for a leading social science journal (ASR, APSR, JCR, AJS).
>
> Review the following manuscript for:
> - **Theoretical contribution**: Does this advance theory meaningfully?
> - **Generalizability**: How do findings extend beyond this sample/context?
> - **Construct validity**: Are concepts measured appropriately?
> - **Sampling strategy**: Is the sample representative for the claims made?
> - **Measurement**: Are measures reliable and valid?
> - **Ethical considerations**: Are there concerns about harm or consent?
> - **Policy implications**: Are practical implications overstated?
>
> Assess both methodological and theoretical contributions.
> Format your response with numbered major and minor comments."

### General Academic

> "Act as a critical peer reviewer for an academic journal.
>
> Review the following manuscript for:
> - **Research question clarity**: Is the central question well-defined?
> - **Methodology appropriateness**: Is the method suited to the question?
> - **Evidence quality**: Do the data support the claims?
> - **Logical flow**: Is the argument coherent and well-structured?
> - **Literature contribution**: What does this add to existing knowledge?
> - **Presentation**: Is the writing clear and accessible?
>
> Be constructive and specific about improvements needed.
> Format your response with numbered major and minor comments."

---

## Response Documentation Template

```markdown
## Review #[N] Response - [DATE]

### Summary Statistics

| Category | Total | Addressed | Beyond Scope | Pending |
|----------|-------|-----------|--------------|---------|
| Major Comments | X | X | X | X |
| Minor Comments | X | X | X | X |

### Major Comments

#### Comment 1: [Title]
**Status**: [VALID - ACTION NEEDED | ALREADY ADDRESSED | BEYOND SCOPE | INVALID]

**Reviewer's Concern**: [Quote or paraphrase]

**Validity Assessment**: [Why this is valid/invalid]

**Response**: [What was done or why not addressed]

**Files Modified**: [List specific files]

---

### Verification Checklist

- [ ] All analyses run successfully
- [ ] Manuscript text updated
- [ ] Tables/figures reflect new results
- [ ] REVISION_TRACKER.md updated
- [ ] Quarto renders without errors
```

---

## CLI Commands

### Check Review Status

```bash
python src/pipeline.py review_status
python src/pipeline.py review_status -m main    # Specific manuscript
```

### Start New Review Cycle

```bash
# SYNTHETIC REVIEWS (focus prompts)
python src/pipeline.py review_new --focus methods
python src/pipeline.py review_new --focus economics
python src/pipeline.py review_new --focus policy
python src/pipeline.py review_new --focus clarity

# Multi-manuscript with focus
python src/pipeline.py review_new -m main -f methods

# ACTUAL REVIEWS (from journals)
python src/pipeline.py review_new --actual --journal "JEEM" --round "R&R1"
python src/pipeline.py review_new --actual -j "AER" -r "initial" --decision major_revision
python src/pipeline.py review_new --actual -j "QJE" -r "R&R1" --reviewers R1 R2 R3
```

**Actual Review Options:**

| Option | Description |
|--------|-------------|
| `--actual` | Mark as actual (journal) review instead of synthetic |
| `--journal, -j` | Journal name (e.g., "JEEM", "AER") |
| `--round, -r` | Submission round (e.g., "initial", "R&R1", "R&R2") |
| `--decision` | Decision received (major_revision, minor_revision, reject, accept) |
| `--reviewers` | Reviewer IDs (e.g., R1 R2 R3) |

### Archive Completed Cycle

```bash
python src/pipeline.py review_archive
python src/pipeline.py review_archive -m main   # Specific manuscript
python src/pipeline.py review_archive --no-tag  # Skip git tagging
python src/pipeline.py review_archive --tag "jeem-r1-final"  # Custom tag
```

Archives the current review cycle and creates a git tag (default: `review-{manuscript}-{cycle:02d}-complete`).

### Generate Visual Diff

```bash
python src/pipeline.py review_diff -m main                    # Current vs previous
python src/pipeline.py review_diff -m main --from 1 --to 2    # Between specific cycles
python src/pipeline.py review_diff -m main --format markdown  # Output format
```

Generates a visual diff showing changes between review cycles.

### Generate Response Letter

```bash
python src/pipeline.py review_response -m main
python src/pipeline.py review_response -m main --format markdown
python src/pipeline.py review_response -m main --include-diffs
```

Generates a "Response to Reviewers" document from the REVISION_TRACKER.md.

### Verify Current Cycle

```bash
python src/pipeline.py review_verify
python src/pipeline.py review_verify -m main    # Specific manuscript
```

Verification includes journal compliance checks:

- Word count (excluding YAML frontmatter and code blocks)
- Self-reference patterns ("this study", "we find")
- Abstract length validation

### Generate Summary Report

```bash
python src/pipeline.py review_report
```

---

## File Organization

| File | Purpose |
|------|---------|
| `doc/SYNTHETIC_REVIEW_PROCESS.md` | This file - methodology guide |
| `doc/MANUSCRIPT_REVISION_CHECKLIST.md` | High-level revision status |
| `doc/reviews/README.md` | Index of all review cycles |
| `doc/reviews/archive/` | Completed review cycles |
| `manuscript_quarto/REVISION_TRACKER.md` | Detailed current review tracking |

---

## Best Practices

1. **Be honest about limitations**: If a concern can't be addressed, say so clearly
2. **Track everything**: Future reviews may raise similar concerns
3. **Verify changes**: Don't assume changes work - test them
4. **Update incrementally**: Small commits, clear messages
5. **Cross-reference**: Link between tracker, manuscript, and code
6. **Learn from patterns**: If multiple reviews raise same issue, it's important

---

## Integration with CLAUDE.md

Key review findings should be incorporated into `CLAUDE.md` project instructions when they affect:
- Default methodology choices
- Interpretation caveats
- Robustness check requirements
- Documentation standards

---

## Review History Template

| Review | Date | Discipline | Major Comments | Addressed | Notes |
|--------|------|------------|----------------|-----------|-------|
| #1 | YYYY-MM-DD | Economics | X | X | Initial review |
| #2 | YYYY-MM-DD | General | X | X | Follow-up |
